{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhav-sa30/Evaluating-LLM-Models-using-TruEra/blob/main/Evaluating_LLM_Models_using_TruEra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **!pip install -U trulens-eval:** This command is used to install or upgrade the Python package named \"trulens-eval\". The -U flag indicates that you want to upgrade the package if it's already installed, and trulens-eval is the name of the package you're installing.\n",
        "\n",
        "2. **!npm install localtunnel -q:** This command, typically used in a Node.js environment, installs the \"localtunnel\" package. \"localtunnel\" is a tool that allows you to expose a local web server to the internet, making it accessible remotely. The -q flag suppresses the output, making the installation process less verbose.\n",
        "\n",
        "3. **!pip install -q streamlit==1.13.0:** This command installs a specific version (1.13.0) of the \"streamlit\" Python package. \"Streamlit\" is a framework used for creating web applications with Python scripts. The -q flag, similar to the previous command, suppresses output during installation."
      ],
      "metadata": {
        "id": "DceSttoVd3tm"
      },
      "id": "DceSttoVd3tm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbZNKt7PdKY9"
      },
      "outputs": [],
      "source": [
        "!pip install -U trulens-eval\n",
        "\n",
        "# Google Colab Dependencies\n",
        "!npm install localtunnel -q\n",
        "!pip install -q streamlit==1.13.0"
      ],
      "id": "mbZNKt7PdKY9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDxHY0LBdKZA"
      },
      "source": [
        "# Langchain Quickstart\n",
        "\n",
        "In this quickstart you will create a simple LLM Chain and learn how to log it and get feedback on an LLM response."
      ],
      "id": "gDxHY0LBdKZA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mJWzTIwdKZC"
      },
      "source": [
        "## Setup\n",
        "### Add API keys\n",
        "For this quickstart you will need Open AI and Huggingface keys"
      ],
      "id": "2mJWzTIwdKZC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- *os* module provides functions for interacting with the operating system\n",
        "- set two environment variables, OPENAI_API_KEY and HUGGINGFACE_API_KEY\n",
        "\n",
        "\\\\\n",
        "####What is environment variable?\n",
        "\n",
        "Environment variables are a way to store data that can be accessed by any program running on the system. In this case, the environment variables are storing your API keys for OpenAI and Huggingface, which are two natural language processing APIs."
      ],
      "metadata": {
        "id": "fzzMtzJGfVow"
      },
      "id": "fzzMtzJGfVow"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPvl4UCFdKZC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\"\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"...\""
      ],
      "id": "gPvl4UCFdKZC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7UIAnBUdKZD"
      },
      "source": [
        "### Import from LangChain and TruLens"
      ],
      "id": "M7UIAnBUdKZD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **IPython.display module** provides functions for displaying data in IPython notebooks\n",
        "- **trulens_eval module** provides functions for evaluating the performance of language models.\n",
        "- trulens_eval module also includes a class called **TruChain**, which represents a language model chain\n",
        "-  The Feedback class is used to collect feedback from users about the performance of a language model chain.\n",
        "- The Huggingface class is used to interact with the Huggingface API.\n",
        "- The Tru class is a helper class that provides some utility functions.\n",
        "- The LLMChain class represents a language model chain that uses an LLM (large language model) as its underlying model.\n",
        "- The OpenAI class is a subclass of LLM that represents an OpenAI LLM.\n",
        "- The ChatPromptTemplate class is a class that represents a chat prompt template.\n",
        "- The HumanMessagePromptTemplate class is a subclass of ChatPromptTemplate that represents a chat prompt template that expects a human message as input."
      ],
      "metadata": {
        "id": "W6WyC0drgfDZ"
      },
      "id": "W6WyC0drgfDZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1-Is6i7dKZD"
      },
      "outputs": [],
      "source": [
        "from IPython.display import JSON\n",
        "\n",
        "# Imports main tools:\n",
        "from trulens_eval import TruChain, Feedback, Huggingface, Tru\n",
        "tru = Tru()\n",
        "\n",
        "# Imports from langchain to build app. You may need to install langchain first\n",
        "# with the following:\n",
        "# ! pip install langchain>=0.0.170\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts.chat import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.prompts.chat import HumanMessagePromptTemplate"
      ],
      "id": "s1-Is6i7dKZD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k38PBciodKZD"
      },
      "source": [
        "### Create Simple LLM Application\n",
        "\n",
        "This example uses a LangChain framework and OpenAI LLM"
      ],
      "id": "k38PBciodKZD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Wk5jQ45dKZE"
      },
      "outputs": [],
      "source": [
        "#We first create a HumanMessagePromptTemplate object, which represents a chat prompt template that expects a human message as input.\n",
        "\n",
        "full_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        template=\n",
        "        \"Provide a helpful response with relevant background information for the following: {prompt}\",  #The {prompt} placeholder is used to indicate where the human message will be inserted.\n",
        "        input_variables=[\"prompt\"],\n",
        "    )\n",
        ")\n",
        "\n",
        "# Next, we create a ChatPromptTemplate object from the HumanMessagePromptTemplate object, which represents a chat prompt that can be used with a language model chain.\n",
        "chat_prompt_template = ChatPromptTemplate.from_messages([full_prompt])\n",
        "\n",
        "# Finally, we create an OpenAI object and an LLMChain object\n",
        "llm = OpenAI(temperature=0.9, max_tokens=128) #The OpenAI object represents an OpenAI LLM (large language model)\n",
        "\n",
        "#The LLMChain object represents a language model chain that uses an LLM as its underlying model.\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=chat_prompt_template, verbose=True)\n",
        "\n",
        "#The LLMChain object is initialized with the llm object, the chat_prompt_template object, and the verbose flag. The verbose flag is set to True to indicate that the chain should print debugging information."
      ],
      "id": "8Wk5jQ45dKZE"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY90oWoEdKZE"
      },
      "source": [
        "### Send your first request"
      ],
      "id": "UY90oWoEdKZE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68VGKIKtdKZE"
      },
      "outputs": [],
      "source": [
        "prompt_input = 'Â¿que hora es?'"
      ],
      "id": "68VGKIKtdKZE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huTxhI2HdKZF"
      },
      "outputs": [],
      "source": [
        "llm_response = chain(prompt_input)\n",
        "\n",
        "display(llm_response)"
      ],
      "id": "huTxhI2HdKZF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7RIUJ3ndKZF"
      },
      "source": [
        "## Initialize Feedback Function(s)"
      ],
      "id": "k7RIUJ3ndKZF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cvHP9QgdKZF"
      },
      "outputs": [],
      "source": [
        "# Initialize Huggingface-based feedback function collection class:\n",
        "hugs = Huggingface()\n",
        "\n",
        "# Define a language match feedback function using HuggingFace.\n",
        "\n",
        "#READ THE DOC AT THIS POINT AND EXPLAIN HOW LANGUAGE MATCH HAPPENS ON INPUT AND OUTPUT\n",
        "\n",
        "f_lang_match = Feedback(hugs.language_match).on_input_output()\n",
        "# By default this will check language match on the main app input and main app\n",
        "# output."
      ],
      "id": "3cvHP9QgdKZF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKPYfg71dKZF"
      },
      "source": [
        "## Instrument chain for logging with TruLens"
      ],
      "id": "uKPYfg71dKZF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbp7V1t5dKZF"
      },
      "outputs": [],
      "source": [
        "truchain = TruChain(chain,\n",
        "    app_id='Chain1_ChatApplication',\n",
        "    feedbacks=[f_lang_match],\n",
        "    tags = \"prototype\")"
      ],
      "id": "lbp7V1t5dKZF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZqaXyE4dKZF"
      },
      "outputs": [],
      "source": [
        "# Instrumented chain can operate like the original:\n",
        "llm_response = truchain(prompt_input)\n",
        "\n",
        "display(llm_response)"
      ],
      "id": "pZqaXyE4dKZF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUB0gzYIdKZG"
      },
      "source": [
        "## Explore in a Dashboard"
      ],
      "id": "pUB0gzYIdKZG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2GoYAkIdKZG"
      },
      "outputs": [],
      "source": [
        "tru.run_dashboard() # open a local streamlit app to explore\n",
        "\n",
        "# tru.stop_dashboard() # stop if needed"
      ],
      "id": "q2GoYAkIdKZG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3ke0RJwdKZG"
      },
      "source": [
        "Alternatively, you can run `trulens-eval` from a command line in the same folder to start the dashboard."
      ],
      "id": "A3ke0RJwdKZG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u8HQjipdKZG"
      },
      "source": [
        "### Chain Leaderboard\n",
        "\n",
        "Understand how your LLM application is performing at a glance. Once you've set up logging and evaluation in your application, you can view key performance statistics including cost and average feedback value across all of your LLM apps using the chain leaderboard. As you iterate new versions of your LLM application, you can compare their performance across all of the different quality metrics you've set up.\n",
        "\n",
        "Note: Average feedback values are returned and displayed in a range from 0 (worst) to 1 (best).\n",
        "\n",
        "![Chain Leaderboard](https://www.trulens.org/Assets/image/Leaderboard.png)\n",
        "\n",
        "To dive deeper on a particular chain, click \"Select Chain\".\n",
        "\n",
        "### Understand chain performance with Evaluations\n",
        "\n",
        "To learn more about the performance of a particular chain or LLM model, we can select it to view its evaluations at the record level. LLM quality is assessed through the use of feedback functions. Feedback functions are extensible methods for determining the quality of LLM responses and can be applied to any downstream LLM task. Out of the box we provide a number of feedback functions for assessing model agreement, sentiment, relevance and more.\n",
        "\n",
        "The evaluations tab provides record-level metadata and feedback on the quality of your LLM application.\n",
        "\n",
        "![Evaluations](https://www.trulens.org/Assets/image/Leaderboard.png)\n",
        "\n",
        "### Deep dive into full chain metadata\n",
        "\n",
        "Click on a record to dive deep into all of the details of your chain stack and underlying LLM, captured by tru_chain.\n",
        "\n",
        "![Explore a Chain](https://www.trulens.org/Assets/image/Chain_Explore.png)\n",
        "\n",
        "If you prefer the raw format, you can quickly get it using the \"Display full chain json\" or \"Display full record json\" buttons at the bottom of the page."
      ],
      "id": "4u8HQjipdKZG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsaQBXXNdKZG"
      },
      "source": [
        "Note: Feedback functions evaluated in the deferred manner can be seen in the \"Progress\" page of the TruLens dashboard."
      ],
      "id": "rsaQBXXNdKZG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri6MIcmndKZG"
      },
      "source": [
        "## Or view results directly in your notebook"
      ],
      "id": "Ri6MIcmndKZG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBBcXj1GdKZG"
      },
      "outputs": [],
      "source": [
        "tru.get_records_and_feedback(app_ids=[])[0] # pass an empty list of app_ids to get all"
      ],
      "id": "DBBcXj1GdKZG"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "d5737f6101ac92451320b0e41890107145710b89f85909f3780d702e7818f973"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}